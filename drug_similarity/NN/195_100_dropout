import tensorflow as tf
import numpy as np
sess = tf.InteractiveSession()

train_steps=100
save_steps=10

#Plugging in the x and y datasets
x = tf.placeholder("float", [None, 195])
y = tf.placeholder("float", [None, 2 ])

#Defining weight variables
def weight_variable(shape,var_name):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial,name=var_name)
  
def bias_variable(shape,var_name):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial, name=var_name)
  
#First Layer (100 Nodes)
w_l1=weight_variable([195,100],'w_l1')
b_l1=bias_variable([100],'b_l1')
h_1=tf.sigmoid(tf.matmul(x,w_l1) + b_l1)

#Dropout
keep_prob = tf.placeholder("float")
h_1_drop = tf.nn.dropout(h_1, keep_prob)

#Readout Layer (Binary output)
w_l2=weight_variable([100,2],'w_l2')
b_l2=bias_variable([2],'b_l2')
y_model=tf.nn.softmax(tf.matmul(h_1_drop,w_l2) + b_l2)

saver = tf.train.Saver()

#Training the model
loss = tf.reduce_mean(tf.square(y - y_model))
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)
correct_prediction = tf.equal(tf.argmax(y_model,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))

#Running the iterations
sess.run(tf.initialize_all_variables())
for i in range(train_steps):
  sess.run(train_step, feed_dict={x: x_data, y: y_data, keep_prob: 0.5})
  if i % save_steps == 0:
    saver.save(sess, 'my-model', global_step=i)
  
  
print sess.run(accuracy, feed_dict={x: x_data, y: y_data, keep_prob: 1.0})

